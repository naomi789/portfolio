{% extends "base.html" %}

{% block content %}

<!--BANNER IMAGE-->
<div class="row">
    <div class="banner-full-img-container">
        <img class="img-fluid" src="{{ url_for('static', filename='images/templates/rectangle.png') }}">
    </div>
</div>
<div class="row">
    <div class="col-2"></div>
    <div class="col-8">
        <!-- CONTENT-->
        <h2 class="proj-title">Clarifying School Ratings</h2>
        <p class="proj-subtitle">
            <b>Problem statement</b>
            Each year, over two million parents in the US use ratings on GreatSchools.org to evaluate school quality and
            help them make important decisions about their children’s education. If the ratings are unclear or
            misinterpreted, families may make decisions based on incomplete or incorrect information.
            To improve user trust and satisfaction, this research focused on identifying which aspects of the school
            ratings system cause confusion and testing solutions to simplify and clarify school ratings for a better
            user experience.
        </p>
        <div class="proj-section-header">Overview - the Research process</div>
        <div class="proj-section-content">
            <ol>
                <li><b>Advocate for research.</b> List known pain points and persuade stakeholders we should do
                    research.
                </li>
                <li><b>Identify research questions. </b>Hypothesize what a "usable" experience would look like, and
                    confirm with the product and data science teams.
                </li>
                <li><b>Design and run research. </b>Draft the research plan, run the initial round of research, and
                    adjust study plan as needed.
                </li>
                <li><b>Share findings and create impact. </b>After analyzing the findings and sharing them out, continue
                    to follow-up with stakeholders as needed.
                </li>
            </ol>
            <p></p>
        </div>
        <div class="proj-section-header">Advocate for research</div>
        <div class="proj-section-content">
            <p>In previous conversations with parents and other users about how they search for K-12 schools, we had
                noticed misconceptions and misinterpretations of school quality information.</p>
            <ul>
                <li>The Customer Service team got tickets from customers complaining that their school was being
                    "dinged" for a certain attribute - when in reality, that attribute wasn't included in the rating
                    calculation.
                </li>
                <li>In interviews, users knew that a higher rating was better, but didn't understand how to interpret
                    ratings. Does a 9/10 rating for test scores mean that the average student gets 90% on a test, or
                    that 90% of students pass the test? Also, are we talking about weekly spelling tests, state
                    proficiency tests, or national exams like the SAT, ACT, IB, and AP tests?
                </li>
                <li></li>
            </ul>
        </div>

        <div class="proj-section-header">Identify research questions</div>
        <div class="proj-section-content">
            <ol>
                <li>Do users understand that an overall score is calculated using only some of the data on a school's
                    profile?
                </li>
                <li>Do users correctly interpret the information we share?</li>
                <li>What follow-up questions do users have about school ratings?</li>
                <li>Is this new prototype usable?</li>
            </ol>
        </div>

        <div class="proj-section-header">Design and run research</div>
        <div class="proj-section-content">
            <p>Parents of K-12 students in the US who recently picked a new K-12 school</p>
            <p>Parents of K-12 students in the US who recently looked up ratings for their kids current school</p>
            <p>Moderated calls on UserTesting.com, 45 minutes each</p>
        </div>
        <div class="proj-section-subheader">Phase 1: Progressive disclosure exercise (n=5)</div>
        <div class="proj-section-content">
            <p>Content</p>
        </div>
        <div class="proj-section-subheader">Phase 2: A/B usability test (n=8)</div>
        <p>Content</p>
        <div class="proj-section-subheader">Phase 3: Usability test (esp. on percentiles) (n=5)</div>
        <p>Content</p>

        <div class="proj-section-header">Share findings and create impact</div>
        <div class="proj-section-content">
            <p>
                All participants' primary question was “what does this rating mean” (what the score represents), and all
                participants felt like the answer to that question should also answer “what data was used” (name of
                test, subject of test, grades of students tested) and “how old the data is” (at a minimum: whether it is
                the newest available data, and ideally also the school year the data is from).
            </p>
            <p>
                For test scores, some wanted to see the average score by grade and/or by subject.
            </p>
            <p>
                For student progress, most wanted to know if the same test was offered twice during one school year
                (although it seems like some participants were asking that because they actually wanted to know if the
                same students were taking the same test twice or not - e.g., what happens with a 5th → 6th grade
                transition. Further research would be needed to understand if the answers to both questions are
                necessary).
            </p>
            <p>
                Although “how should the score should be interpreted” was not an option during the activity, it should
                have been. Many participants misinterpreted rating(s) by reading the rating in the context of their
                student (rather than in the context of how the school is doing in the state). In other words, when
                participants looked at a rating, they were asking themself “What does this rating mean for my child?”
            </p>
        </div>

        <!--        <div class="row">-->
        <!--            <div class="col-3">-->
        <!--                <h3 class="fact-title">Role</h3>-->
        <!--                <p class="fact-fact">UX Researcher</p>-->
        <!--            </div>-->
        <!--            <div class="col-3">-->
        <!--                <h3 class="fact-title">Company</h3>-->
        <!--                <p class="fact-fact">GreatSchools.org</p>-->
        <!--            </div>-->
        <!--            <div class="col-3">-->
        <!--                <h3 class="fact-title">Timeframe</h3>-->
        <!--                <p class="fact-fact">Fall 2024</p>-->
        <!--            </div>-->
        <!--            <div class="col-3">-->
        <!--                <h3 class="fact-title">Teammates</h3>-->
        <!--                <p class="fact-fact">Designer</p>-->
        <!--                <p class="fact-fact">Product Manager</p>-->
        <!--                <p class="fact-fact">Data Scientist</p>-->
        <!--            </div>-->
        <!--        </div>-->


        <div class="proj-section-header">Limitations</div>
        <div class="proj-section-content">
            <!-- <p>(The difficult thing you had to overcome) client hated research, users were hard to come by, data was all screwed up</p> -->
            <p>Can't add more data, can't change how the score is calculated</p>
            <p>Stakeholder resistance to re-naming/re-branding the "equity" rating</p>
        </div>
        <div class="proj-section-header">Future directions</div>
        <div class="proj-section-content">
            <p>INSERT</p>

            <p>INSERT</p>
        </div>
    </div>
    <div class="col-2"></div>
</div>
{% endblock %}
